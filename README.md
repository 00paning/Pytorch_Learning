# Pytorch_Learning
  记录学习pytorch的相关经历，更新ing
## 1.Linear Moedel
- 由于是初级阶段，这里采用穷举法对w进行取值，观察图像看不同w对应的mse的变化情况  
## 2.梯度下降法  
- 下面是关于梯度下降法的初始公式，具体的推导可以相应的求偏导即可  
- 首先gradient是每次训练时的cost对w求偏导,alpha表示学习率，更新的w等于每次的gradient乘以学习率  
$$w = w - \alpha \frac {dcost}{dw}$$  
## 3.随机梯度下降  
- 由于有鞍点的存在，在训练的过程中如果遇到鞍点根据上面梯度下降法的公式发现，这种情况会导致w的值不发生变化，这时可以用随机梯度下降法来解决这个问题。
这个方法相比与梯度下降法的差别是，在计算loss的时候不采取使用全部的点的loss，而是随机的选取一个点的loss进行计算
- 随机梯度下降在计算每个x的时候不能和梯度下降法一样可以并行计算，所以运行的效率很慢  
## 4.反向传播  
## 5.pytorch实现线性回归  
w= 1.999743938446045  
b= 0.0005822281818836927  
y_pred= tensor([[7.9996]])  
## 6.逻辑回归  
## 7.处理多维数据输入  
- 这里用的糖尿病分类数据集，同时通过多个线性层揭示了神经网络的雏形  
